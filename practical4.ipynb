{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-1KqPevmgvkj"
      },
      "source": [
        "# Computer Vision HT 2025 - Optional Practical 4 (v1.0)\n",
        "\n",
        "**This practical is entirely optional.**\n",
        "\n",
        "## Instructions\n",
        "\n",
        "1. Start Google Colab: https://colab.research.google.com. A modal dialog should have appeared to open a new notebook. If not, go to \"File>Open notebook\".\n",
        "2. From the open notebook dialog, select the GitHub \"tab\" and enter this URL: https://github.com/chrirupp/cv_course\n",
        "3. The notebook(s) should appear (*.ipynb). Select the one for the current practical.\n",
        "4. To run a notebook on Colab you will typically need some data files (e.g., images). As Colab only loads the notebook itself, these other files need to be downloaded separately. The following cell is a `%%sh` block that downloads the required files. You can inspect the downloaded files by clicking on the \"Files\" tab on the left.\n",
        "\n",
        "## Practicalities\n",
        "\n",
        "The signing-off happens in the last half hour of each session or at the beginning of the following one.\n",
        "As usual, when checking your work the demonstrator will want to see a working version of the program in action, as well as appropriate comments of your code. Try to make your report as concise as possible, perhaps in the form of appropriate comments to your code.\n",
        "\n",
        "Since this is a new practical task, any errors, ambiguities or suggestions for improvement should be flagged as soon as possible.\n",
        "\n",
        "If you are not familiar with the way practicals run, there are department-wide [rules](https://www.cs.ox.ac.uk/teaching/courses/2023-2024/practicals/). There you will find how the compulsory part, the optional tasks, and your report will factor into your mark.\n",
        "\n",
        "## Advice\n",
        "\n",
        "* You will need to look at the code for the lectures. There you will find many related computations that you can reuse and adapt to solve the practicals.\n",
        "* The compulsory part of this practical is designed to give you additional understanding of the concepts taught in the lectures. It should be achievable in one session."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L9ZIrTQ7gvkl"
      },
      "source": [
        "Here we import some libraries that we will need to process images, do maths, and to visualise results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oSsA7c8ngvkm"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import torchvision\n",
        "import torch\n",
        "import cv2\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "import json\n",
        "from torchvision import transforms\n",
        "from tqdm import tqdm\n",
        "from torchvision import datasets, transforms\n",
        "from IPython.display import display, clear_output\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PbqaaCMagvko"
      },
      "source": [
        "The usual set of helper functions:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class Visualizer():\n",
        "    def __init__(self, num_rows=1, num_cols=1, figsize=(5,5), axis_off=True, title='', tight=False, cm=None):\n",
        "        self.fig, self.axs = plt.subplots(num_rows, num_cols, figsize=figsize, squeeze=False)\n",
        "        # remove ticks\n",
        "        if axis_off:\n",
        "          plt.setp(plt.gcf().get_axes(), xticks=[], yticks=[])\n",
        "        # set colormap\n",
        "        if cm is not None:\n",
        "            plt.set_cmap(cm)\n",
        "        # set supertitle\n",
        "        self.fig.suptitle(title)\n",
        "        if tight:\n",
        "            self.fig.subplots_adjust(top=0.88)\n",
        "\n",
        "    def add_image_subplot(self, i, j, image, normalize=False, title_str=''):\n",
        "        if normalize:\n",
        "            image = self.normalize_image(image)\n",
        "        if len(image.shape) == 3:\n",
        "            #BGR -> RGB\n",
        "            image = image[:, :, ::-1]\n",
        "        self.axs[i, j].imshow(image)\n",
        "        self.axs[i, j].set_title(title_str)\n",
        "\n",
        "    def show(self):\n",
        "        display(self.fig)\n",
        "        clear_output(wait = True)\n",
        "        plt.pause(0.05)\n",
        "\n",
        "    def add_stem_subplot(self, i, j, x, y, title_str=''):\n",
        "        self.axs[i, j].stem(x, y)\n",
        "        self.axs[i, j].set_title(title_str)\n",
        "\n",
        "    def add_subplot(self, i, j, data, title_str=''):\n",
        "        self.axs[i, j].plot(data)\n",
        "        self.axs[i, j].set_title(title_str)\n",
        "\n",
        "    def add_subplot_xy(self, i, j, x, y, title_str=''):\n",
        "        self.axs[i, j].plot(x,y)\n",
        "        self.axs[i, j].set_title(title_str)\n",
        "\n",
        "    def add_bar_subplot(self, i, j, x, y, title_str=''):\n",
        "        self.axs[i, j].bar(x, y)\n",
        "        self.axs[i, j].set_title(title_str)\n",
        "\n",
        "    @staticmethod\n",
        "    def normalize_image(image):\n",
        "        img = np.float64(image) - np.min(image)\n",
        "        img /= np.max(img)\n",
        "        return img"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F4AVSwOFgvkp"
      },
      "outputs": [],
      "source": [
        "%%sh\n",
        "# Download the data - you need to do this only once\n",
        "wget --no-verbose --output-document=./image_cc3.jpg https://github.com/chrirupp/cv_course/raw/main/data/image_cc3.jpg"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Problem 4.1 - Coordinate Networks\n",
        "\n",
        "We have seen that neural rendering (e.g. NeRFs) learn a coordinate network that maps from coordinates to colours. Here we will train a small model that learns to memorise a single 2D image $f(x,y) = (r,g,b)$.\n",
        "Define a small 3 layer MLP and write a function to sample an image from it. We will normalise pixel coordinates to lie in the range -1 to 1."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "image = cv2.imread('image_cc_3.jpg')\n",
        "#downsample the image by a factor of 8\n",
        "image = cv2.resize(image, (0,0), fx=0.125, fy=0.125)\n",
        "image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "image = torch.tensor(image, dtype=torch.float32).permute(2,0,1)/255.0\n",
        "\n",
        "class CoordinateNetwork(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.fc1 = None \n",
        "        self.fc2 = None\n",
        "        self.fc3 = None\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x\n",
        "    \n",
        "model = CoordinateNetwork()\n",
        "\n",
        "def generate_image(model):\n",
        "    img = torch.zeros_like(image)\n",
        "\n",
        "    # generate one row at a time\n",
        "    for y in range(image.shape[1]):\n",
        "        y_coordinates = None\n",
        "        x_coordinates = None\n",
        "        pixel_coordinates = torch.cat([x_coordinates, y_coordinates], dim=1)\n",
        "        with torch.no_grad():\n",
        "            colors = model(pixel_coordinates)\n",
        "        img[:, y, :] = colors.permute(1,0).detach().cpu()\n",
        "    \n",
        "    return torch.clip(img, 0, 1)\n",
        "\n",
        "gen_image = generate_image(model)\n",
        "vis = Visualizer(num_rows=1, num_cols=2, figsize=(10,5), axis_off=True)\n",
        "vis.add_image_subplot(0, 0, image.permute(1,2,0).numpy()[:, :, ::-1], title_str='Target image')\n",
        "vis.add_image_subplot(0, 1, gen_image.permute(1,2,0).numpy()[:, :, ::-1], title_str='Generated image')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Load `image_cc3.jpg` and train it to reconstruct the image. For this, randomly sample a batch of pixel coordinates. You can use `torch.grid_sample` to find the colors for these pixels. This support bilinear interpolation so you do not need to sample exact pixel coordinates."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def train(model, optimizer, iterations=10000):\n",
        "    bar = tqdm(range(iterations))\n",
        "    for _ in bar:\n",
        "        pixel_coordinates = None\n",
        "        target_colors = None\n",
        "        pred_colors = model(pixel_coordinates)\n",
        "        loss = torch.mean(torch.abs(target_colors - pred_colors))\n",
        "\n",
        "        bar.set_description(f'Loss: {loss.item()}', refresh=False)\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "model = CoordinateNetwork()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "for steps in range(100):\n",
        "    train(model, optimizer, iterations=10000)\n",
        "    gen_image = generate_image(model)\n",
        "    vis = Visualizer(num_rows=1, num_cols=2, figsize=(10,5), axis_off=True)\n",
        "    vis.add_image_subplot(0, 0, image.permute(1,2,0).numpy()[:, :, ::-1], title_str='Target image')\n",
        "    vis.add_image_subplot(0, 1, gen_image.permute(1,2,0).numpy()[:, :, ::-1], title_str='Generated image')\n",
        "    vis.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The generated image is very bad and blurry as the network has difficulties learning high frequency components. Positional encoding comes to our rescue. Define a second network that embeds the pixels coordinates using multiple octaves of sin and cos. Separately encode each channel using a positional encoding. The lowest frequency should be `torch.pi`, and each frequency thereafter should be double the previous frequency. For each frequency, you should encode the input signal using both sine and cosine. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class CoordinateNetworkPE(nn.Module):\n",
        "    def __init__(self, num_octaves=6):\n",
        "        super().__init__()\n",
        "        self.num_octaves = num_octaves\n",
        "        # todo - precompute as much as you can\n",
        "\n",
        "        self.fc1 = None\n",
        "        self.fc2 = None\n",
        "        self.fc3 = None\n",
        "\n",
        "    def forward(self, x):\n",
        "        \n",
        "        # todo - implement positional encoding\n",
        "        x = None\n",
        "        \n",
        "        # todo - implement forward pass\n",
        "        return x\n",
        "    \n",
        "model = CoordinateNetworkPE()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "for steps in range(100):\n",
        "    train(model, optimizer, iterations=10000)\n",
        "    gen_image = generate_image(model)\n",
        "    vis = Visualizer(num_rows=1, num_cols=2, figsize=(10,5), axis_off=True)\n",
        "    vis.add_image_subplot(0, 0, image.permute(1,2,0).numpy()[:, :, ::-1], title_str='Target image')\n",
        "    vis.add_image_subplot(0, 1, gen_image.permute(1,2,0).numpy()[:, :, ::-1], title_str='Generated image')\n",
        "    vis.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Play around with the MLP size, number of octaves, learning rate, loss function, etc. \n",
        "\n",
        "* Which configuration give the best results?\n",
        "* Why does the model learn a grayscale image first?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Problem 4.2 - RotNet\n",
        "\n",
        "In practical 2 you have trained a classifier on CIFAR10. Now train an unsupervised network using only the images. We will follow the idea of [RotNet](https://arxiv.org/pdf/1803.07728.pdf) and learn to predict rotations.\n",
        "To do this, you will need to randomly rotate the images in your batch by multiples of 90 degress (this gives you 4 classes) and use the rotation as the class label.\n",
        "\n",
        "To evaluate your model, you can implement nearest neighbour lookup: For each validation sample check which training sample is closest in feature space. Then use that training sample's label to classify the validation set. This is slightly unrealistic (if we had training labels we could have trained with supervision) but a good baseline to understand the quality of your embedding space.\n",
        "\n",
        "For fast NN lookup you can use the `faiss` library. It needs these steps to install on Colab.\n",
        "```\n",
        "!apt install libomp-dev\n",
        "!python -m pip install --upgrade faiss faiss-gpu\n",
        "import faiss\n",
        "```\n",
        "\n",
        "Compare the performance of your classifier to an untrained feature learning network and your supervised model from practical 2. \n",
        "\n",
        "You can start with this supervised template:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# get cifar10 dataset\n",
        "transform = transforms.Compose([transforms.ToTensor()])\n",
        "train_dataset = datasets.CIFAR10('../data', train=True, download=True, transform=transform)\n",
        "test_dataset = datasets.CIFAR10('../data', train=False, download=True, transform=transform)\n",
        "\n",
        "# create dataloaders\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
        "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=128, shuffle=False)\n",
        "\n",
        "def evaluate(model, test_loader):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    with torch.no_grad():\n",
        "        for data, target in test_loader:\n",
        "            output = model(data)\n",
        "            pred = output.argmax(dim=1, keepdim=True)\n",
        "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "\n",
        "    accuracy = 100. * correct / len(test_loader.dataset)\n",
        "    return accuracy\n",
        "\n",
        "def train(model, train_loader, test_loader, criterion, optimizer, num_epochs=10):\n",
        "    train_losses = []\n",
        "    test_accuracies = []\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        for data, target in tqdm(train_loader):\n",
        "            optimizer.zero_grad()\n",
        "            output = model(data)\n",
        "            loss = criterion(output, target)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            train_losses.append(loss.item())\n",
        "\n",
        "        test_acc = evaluate(model, test_loader)\n",
        "        test_accuracies.append(test_acc)\n",
        "        print(f'Epoch {epoch+1}/{num_epochs}, Test accuracy: {test_acc:.2f}%')\n",
        "\n",
        "    return train_losses, test_accuracies\n",
        "\n",
        "class MiniCNN(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.conv1 = torch.nn.Conv2d(3, 16, kernel_size=3, stride=1, padding=1)\n",
        "        self.conv2 = torch.nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1)\n",
        "        self.fc = torch.nn.Linear(4096//2, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.nn.functional.relu(self.conv1(x))\n",
        "        x = torch.nn.functional.max_pool2d(x, kernel_size=2, stride=2)\n",
        "        x = torch.nn.functional.relu(self.conv2(x))\n",
        "        x = torch.nn.functional.max_pool2d(x, kernel_size=2, stride=2)\n",
        "        x = x.view(-1, 4096//2)\n",
        "        x = self.fc(x)\n",
        "        return x\n",
        "    \n",
        "model = MiniCNN()\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "train_losses, test_accuracies = train(model, train_loader, test_loader, criterion, optimizer, num_epochs=5)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
