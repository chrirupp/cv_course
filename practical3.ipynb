{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-1KqPevmgvkj"
      },
      "source": [
        "# Computer Vision HT 2025 - Practical 3 (v1.0)\n",
        "\n",
        "## Instructions\n",
        "\n",
        "1. Start Google Colab: https://colab.research.google.com. A modal dialog should have appeared to open a new notebook. If not, go to \"File>Open notebook\".\n",
        "2. From the open notebook dialog, select the GitHub \"tab\" and enter this URL: https://github.com/chrirupp/cv_course\n",
        "3. The notebook(s) should appear (*.ipynb). Select the one for the current practical.\n",
        "4. To run a notebook on Colab you will typically need some data files (e.g., images). As Colab only loads the notebook itself, these other files need to be downloaded separately. The following cell is a `%%sh` block that downloads the required files. You can inspect the downloaded files by clicking on the \"Files\" tab on the left.\n",
        "\n",
        "## Practicalities\n",
        "\n",
        "The signing-off happens in the last half hour of each session or at the beginning of the following one.\n",
        "As usual, when checking your work the demonstrator will want to see a working version of the program in action, as well as appropriate comments of your code. Try to make your report as concise as possible, perhaps in the form of appropriate comments to your code.\n",
        "\n",
        "Since this is a new practical task, any errors, ambiguities or suggestions for improvement should be flagged as soon as possible.\n",
        "\n",
        "If you are not familiar with the way practicals run, there are department-wide [rules](https://www.cs.ox.ac.uk/teaching/courses/2023-2024/practicals/). There you will find how the compulsory part, the optional tasks, and your report will factor into your mark.\n",
        "\n",
        "## Advice\n",
        "\n",
        "* You will need to look at the code for the lectures. There you will find many related computations that you can reuse and adapt to solve the practicals.\n",
        "* The compulsory part of this practical is designed to give you additional understanding of the concepts taught in the lectures. It should be achievable in one session."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L9ZIrTQ7gvkl"
      },
      "source": [
        "Here we import some libraries that we will need to process images, do maths, and to visualise results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oSsA7c8ngvkm"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import torchvision\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "import json\n",
        "from torchvision import transforms\n",
        "from tqdm import tqdm\n",
        "from torchvision import datasets, transforms\n",
        "from IPython.display import display, clear_output\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PbqaaCMagvko"
      },
      "source": [
        "The usual set of helper functions:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F4AVSwOFgvkp"
      },
      "outputs": [],
      "source": [
        "%%sh\n",
        "# Download the data - you need to do this only once\n",
        "wget --no-verbose --output-document=./data/coco_bboxes.json https://github.com/chrirupp/cv_course/raw/main/data/coco_bboxes.json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4rYxXpSAgvkq"
      },
      "outputs": [],
      "source": [
        "class Visualizer():\n",
        "    def __init__(self, num_rows=1, num_cols=1, figsize=(5,5), axis_off=True, title='', tight=False, cm=None):\n",
        "        self.fig, self.axs = plt.subplots(num_rows, num_cols, figsize=figsize, squeeze=False)\n",
        "        # remove ticks\n",
        "        if axis_off:\n",
        "          plt.setp(plt.gcf().get_axes(), xticks=[], yticks=[])\n",
        "        # set colormap\n",
        "        if cm is not None:\n",
        "            plt.set_cmap(cm)\n",
        "        # set supertitle\n",
        "        self.fig.suptitle(title)\n",
        "        if tight:\n",
        "            self.fig.subplots_adjust(top=0.88)\n",
        "\n",
        "    def add_image_subplot(self, i, j, image, normalize=False, title_str=''):\n",
        "        if normalize:\n",
        "            image = self.normalize_image(image)\n",
        "        if len(image.shape) == 3:\n",
        "            #BGR -> RGB\n",
        "            image = image[:, :, ::-1]\n",
        "        self.axs[i, j].imshow(image)\n",
        "        self.axs[i, j].set_title(title_str)\n",
        "\n",
        "    def show(self):\n",
        "        display(self.fig)\n",
        "        clear_output(wait = True)\n",
        "        plt.pause(0.05)\n",
        "\n",
        "    def add_stem_subplot(self, i, j, x, y, title_str=''):\n",
        "        self.axs[i, j].stem(x, y)\n",
        "        self.axs[i, j].set_title(title_str)\n",
        "\n",
        "    def add_subplot(self, i, j, data, title_str=''):\n",
        "        self.axs[i, j].plot(data)\n",
        "        self.axs[i, j].set_title(title_str)\n",
        "\n",
        "    def add_subplot_xy(self, i, j, x, y, title_str=''):\n",
        "        self.axs[i, j].plot(x,y)\n",
        "        self.axs[i, j].set_title(title_str)\n",
        "\n",
        "    def add_bar_subplot(self, i, j, x, y, title_str=''):\n",
        "        self.axs[i, j].bar(x, y)\n",
        "        self.axs[i, j].set_title(title_str)\n",
        "\n",
        "    @staticmethod\n",
        "    def normalize_image(image):\n",
        "        img = np.float64(image) - np.min(image)\n",
        "        img /= np.max(img)\n",
        "        return img"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q9h1tUwxgvkq"
      },
      "source": [
        "## Problem 3.1 - Object Detection\n",
        "\n",
        "Let us build an evaluation function for an object detector. Assume that for each image we are given bounding boxes by their top-left $(x_1,y_1)$ and bottom-right $(x_2,y_2)$ together with a confidence $c \\in \\mathbb{R}^C$.\n",
        "\n",
        "We will start by computing the intersection over union of two boxes.\n",
        "Hint: the area of the union of two boxes can also be computed as: area1 + area2 - intersection."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sNBkpXMWgvkr",
        "outputId": "c7fc65cb-67f9-42e1-a5bc-ee3803c9d34e"
      },
      "outputs": [],
      "source": [
        "def iou(box1, box2):\n",
        "    # box = (x1, y1, x2, y2, c) with (x1, y1) being the top left corner and (x2, y2) the bottom right corner, ignore the class c for IoU\n",
        "    # determine the (x, y)-coordinates of the intersection rectangle\n",
        "    iou = 0\n",
        "    # return the intersection over union value\n",
        "    return iou\n",
        "\n",
        "# test the iou function\n",
        "box1 = (50, 50, 100, 100, 1.0)\n",
        "box2 = (60, 60, 110, 110, 1.0)\n",
        "box3 = (101, 101, 105, 105, 0.5)\n",
        "print(iou(box1, box1))  # should be 1\n",
        "print(iou(box1, box3))  # should be 0\n",
        "print(iou(box1, box2))  # should be ca. 0.4774\n",
        "print(iou(box2, box3))  # should be ca. 0.0096"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zhm0i-Avgvks"
      },
      "source": [
        "Let us now compute the precision and recall for a given confidence threshold. For this problem we will load some real predictions and GT annotations from `coco_bboxes.json`.\n",
        "This is a real file from a model we are currently researching. We have reduced this file to only a few images and only a single class \"person\" to make this exercise easier.\n",
        "\n",
        "You are loading a nested data structure of the following form:\n",
        "\n",
        "* The data is given as a list of images `[image]`.\n",
        "* Each image is a dictionary with three elements: \n",
        "  * `image[\"image_id\"]` is the filename of the image - you can ignore this.\n",
        "  * `image[\"gt_boxes\"]` is as list of ground truth boxes $(x,y,w,h) \\in mathbb{R}^4$.\n",
        "  * `image[\"pred_boxes\"]` is as list of predictions $(\\hat{x},\\hat{y},\\hat{w},\\hat{h},\\hat{c}) \\in mathbb{R}^5$.\n",
        "\n",
        "All coordinates and sizes have been normalised by their respective image size, so that you do not need to take this into account.\n",
        "We are again looking only at a single class, thus predictions only have a scalar confidence and GT boxes do not have a class at all.\n",
        "\n",
        "Write a function that computes the IoU between each predicted and GT box in an image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def compute_ious(gt_boxes, pred_boxes):\n",
        "    ious = np.zeros((len(gt_boxes), len(pred_boxes)))\n",
        "    # TODO: compute pairwise IoUs\n",
        "    return ious\n",
        "\n",
        "# load the file\n",
        "with open('./data/coco_bboxes.json', 'r') as f:\n",
        "    data = json.load(f)\n",
        "    \n",
        "print(compute_ious(data[0]['gt_boxes'], data[0]['pred_boxes']))  # you should see how the predictions and gt boxes will likely match"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We will now precompute the IoU scores for each image, so that we can later quickly evaluate different thresholds.\n",
        "For the greedy matching in the next step, we need to sort the predicted boxes by decreasing confidence before computing IoU.\n",
        "make sure to update the `data` so that the sorting of IoUs matches the data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def compute_all_ious(data):\n",
        "    all_ious = []\n",
        "    # TODO sort and compute IoUs\n",
        "    return all_ious\n",
        "\n",
        "all_ious = compute_all_ious(data)\n",
        "print(all_ious[1])  # for the second image, you should see that each gt has an almost perfectly matching prediction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now to compute the TP, FP, FN for an image, we perform greedy matching between GT boxes and predictions.\n",
        "You will need to keep a lookup table if a GT box had already been matched.\n",
        "Iterate over all predicted boxes in decreasing confidence, discarding boxes below the confidence threshold.\n",
        "Now, for each predicted box, find the GT box with highest overlap that has not been matched.\n",
        "Count the best match as TP if it is $\\ge$ than the IoU threshold.\n",
        "\n",
        "Your function should return TP, Fp and FN counts. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def compute_tp_fp_fn(ious, pred_boxes, th_confidence, th_iou):\n",
        "    tp = 0\n",
        "    fp = 0\n",
        "    fn = ious.shape[0]\n",
        "    # TODO: compute and count matches\n",
        "    return tp, fp, fn\n",
        "\n",
        "# test the function\n",
        "tp, fp, fn = compute_tp_fp_fn(all_ious[0], data[0]['pred_boxes'], 0.0, 0.5)\n",
        "print(tp, fp, fn)  # you should see 2 1 0 for confidence 0.0 and iou 0.5"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We now need to accumulate TP, FP, FN across the evaluation dataset to compute precision and recall."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def precision_recall(all_ious, data, th_confidence, th_iou):\n",
        "    tp = 0\n",
        "    fp = 0\n",
        "    fn = 0\n",
        "    # TODO: accumulate across images\n",
        "    # TODO: compute precision and recall\n",
        "    precision = 0\n",
        "    recall = 0\n",
        "    return precision, recall\n",
        "\n",
        "# test the function\n",
        "precision, recall = precision_recall(all_ious, data, 0.0, 0.5)\n",
        "print(precision, recall)  # about 0.52 and 0.94"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Plot a precision-recall curve for our detector by iterating over 100 confidence thresholds between 0 and 1.\n",
        "You might need to modify your code above to deal with situations where a high confidence threshold eliminates all predictions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "precision_values = []\n",
        "recall_values = []\n",
        "\n",
        "# TODO: fill precision/recall lists\n",
        "\n",
        "viz = Visualizer(num_rows=1, num_cols=1, figsize=(5,5), axis_off=False, title='Precision and Recall', tight=True)\n",
        "viz.add_subplot_xy(0, 0, recall_values, precision_values)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Analyse the PR curve for several IoU thresholds.\n",
        "* What can you tell about our detector?\n",
        "\n",
        "### Optional Task - overconfidence is a slow and insidious killer\n",
        "Another way to analyse the quality of the confidence prediction is to plot a scatter plot using all predicted boxes. The the x-axis is the confidence, while the y-axis is the IoU with the best matching GT box. If this plot is close to a line (x=y), the confidence correlated with the expected GT box overlap. \n",
        "Create this plot and draw some conclusions.\n",
        "\n",
        "### Optional Task - Scaling up the evaluation\n",
        "Download `coco_bboxes_full.json` and extend your evaluation code to handle multiple classes. Average precision and recall across all 80 classes of the COCO dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WsV_a88qgvkt"
      },
      "source": [
        "## Problem 3.2 - Diffusion Models\n",
        "\n",
        "We will now train a tiny diffusion model on the car images in CIFAR 10. The architecture is a small U-Net:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1PnIdEsqgvku"
      },
      "outputs": [],
      "source": [
        "class Model(nn.Module):\n",
        "    def __init__(self, in_channels=3, out_channels=3):\n",
        "        super().__init__()\n",
        "        self.down_layers = torch.nn.ModuleList([ \n",
        "            nn.Conv2d(in_channels, 32, kernel_size=5, padding=2),\n",
        "            nn.Conv2d(32, 64, kernel_size=5, padding=2),\n",
        "            nn.Conv2d(64, 64, kernel_size=5, padding=2),\n",
        "        ])\n",
        "        self.up_layers = torch.nn.ModuleList([\n",
        "            nn.Conv2d(64, 64, kernel_size=5, padding=2),\n",
        "            nn.Conv2d(128, 32, kernel_size=5, padding=2),\n",
        "            nn.Conv2d(64, 32, kernel_size=5, padding=2), \n",
        "        ])\n",
        "        self.final_conv1 = nn.Conv2d(32, 16, kernel_size=3, padding=1)\n",
        "        self.final_conv2 = nn.Conv2d(16, out_channels, kernel_size=3, padding=1)\n",
        "        self.act = nn.LeakyReLU() \n",
        "        self.downscale = nn.MaxPool2d(2)\n",
        "        self.upscale = nn.Upsample(scale_factor=2)\n",
        "\n",
        "    def forward(self, x, timestep):\n",
        "        skips = []\n",
        "        for i, layer in enumerate(self.down_layers):\n",
        "            x = self.act(layer(x))\n",
        "            if i < 2: \n",
        "              skips.append(x)\n",
        "              x = self.downscale(x)\n",
        "              \n",
        "        for i, layer in enumerate(self.up_layers):\n",
        "            if i > 0: \n",
        "              x = self.upscale(x) \n",
        "              x = torch.concatenate( [x, skips.pop()], axis=1) \n",
        "            x = self.act(layer(x)) \n",
        "        \n",
        "        x = self.act(self.final_conv1(x))\n",
        "        x = self.final_conv2(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8aHsinQ2gvku"
      },
      "source": [
        "**Task:** Draw a diagram of the U-Net architecture to understand the connections within the network.\n",
        "\n",
        "We will now load and filter the dataset to contain only cars (class_id = 1). You can use `torch.utils.data.Subset` to obtain a subset of a dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MhcEMfOKgvku",
        "outputId": "880949b3-088f-4df8-970a-279aba970a9e"
      },
      "outputs": [],
      "source": [
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
        "])\n",
        "cifar_train = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
        "\n",
        "# TODO: filter training imgs\n",
        "cifar_train_reduced = None \n",
        "\n",
        "BATCH_SIZE = 128\n",
        "train_loader = torch.utils.data.DataLoader(cifar_train_reduced, batch_size=BATCH_SIZE, shuffle=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sIS1IGxvgvkv"
      },
      "source": [
        "We will now need to compute a noise schedule. We will precompute the schedule and some useful values to reuse them later.\n",
        "There are three parameters we will get to define the schedule $\\beta_1, \\beta_2$ and $T$.\n",
        "$T$ is the number of steps (we will use 1000)\n",
        "$\\beta_1$ and $\\beta_2$ define the start and end of th enoise schedule.\n",
        "\n",
        "Define a vector $\\beta_t$ for $t \\in \\{ 0, T \\} that starts with $\\beta_1$ and ends with $\\beta_2$. Make sure that $\\beta_t = \\beta_1$ for $t=0$ and $\\beta_t = \\beta_2$ for $t=T$. ($T+1$ values).\n",
        "Also precomupte the following values:\n",
        "* $\\sqrt{\\beta_t}$\n",
        "* $\\alpha_t = 1 - \\beta_t$\n",
        "* $\\bar{\\alpha}_t = \\prod_{i=0}^t \\alpha_i$\n",
        "* $\\sqrt{\\bar{\\alpha}_t}$\n",
        "* $\\frac{1}{\\sqrt{\\alpha_t}}$\n",
        "* $\\sqrt{1 - \\bar{\\alpha}_t}$\n",
        "* $\\frac{1 - \\alpha_t}{\\sqrt{1 - \\bar{\\alpha}_t}}$\n",
        "\n",
        "Then use the following formula to show examples of an image being noise through forward diffusion: $ x_t = \\sqrt{\\bar{\\alpha}_t} x_0 + \\sqrt{1 - \\bar{\\alpha}_t} \\epsilon$, where $\\epsilon \\sim \\mathcal{N}(0,1)$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 917
        },
        "id": "3oxh14rngvkv",
        "outputId": "afc9aecc-e8cc-4303-b664-b81c36a6e2bf"
      },
      "outputs": [],
      "source": [
        "class Schedule():\n",
        "    def __init__(self, beta1, beta2, T, device=\"cpu\"):\n",
        "        super().__init__()\n",
        "        self.device = device\n",
        "        self.num_steps = T\n",
        "\n",
        "        # TODO: create torch vectors containing values for all timesteps\n",
        "        # Hint: use torch.arange(0, T + 1, dtype=torch.float32, device=device) / T to create the betas\n",
        "        self.sqrt_beta_t = 0\n",
        "        self.alpha_t = 0\n",
        "        self.alphabar_t = 0\n",
        "        self.sqrtab = 0\n",
        "        self.oneover_sqrta = 0\n",
        "        self.sqrtmab = 0\n",
        "        self.mab_over_sqrtmab_inv = 0\n",
        "\n",
        "\n",
        "# show images with different noise levels\n",
        "schedule = Schedule(1e-4, 0.02, 1000)\n",
        "\n",
        "# plot the schedule\n",
        "vis = Visualizer(num_rows=2, num_cols=4, figsize=(15, 8), axis_off=False, title='Schedule')\n",
        "vis.add_subplot(0, 0, schedule.sqrt_beta_t, title_str=r'$\\sqrt{\\beta_t}$')\n",
        "vis.add_subplot(0, 1, schedule.alpha_t, title_str=r'$\\alpha_t$')\n",
        "vis.add_subplot(1, 0, schedule.alphabar_t, title_str=r'$\\bar{\\alpha}_t$')\n",
        "vis.add_subplot(1, 1, schedule.sqrtab, title_str=r'$\\sqrt{\\bar{\\alpha}_t}$')\n",
        "vis.add_subplot(0, 2, schedule.oneover_sqrta, title_str=r'$\\frac{1}{\\sqrt{\\alpha_t}}$')\n",
        "vis.add_subplot(0, 3, schedule.sqrtmab, title_str=r'$\\sqrt{1 - \\bar{\\alpha}_t}$')\n",
        "vis.add_subplot(1, 2, schedule.mab_over_sqrtmab_inv, title_str=r'$\\frac{1 - \\alpha_t}{\\sqrt{1 - \\bar{\\alpha}_t}}$')\n",
        "\n",
        "vis = Visualizer(num_rows=1, num_cols=11, figsize=(15, 2), axis_off=True, title='Images with different noise levels')\n",
        "for i in range(11):\n",
        "    img, _ = cifar_train_reduced[0]\n",
        "    noise = torch.randn(img.shape)\n",
        "    step = i * (schedule.num_steps // 10)\n",
        "\n",
        "    # TODO: compute noised sample\n",
        "    img = None\n",
        "    \n",
        "    # normalise for display\n",
        "    img = (img + 1) / 2\n",
        "    img = img.clip(0, 1)\n",
        "    img = img.permute(1, 2, 0)\n",
        "    vis.add_image_subplot(0, i, img.numpy(), title_str=f'Step: {step}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EfBSnFCjgvkv"
      },
      "source": [
        "We now need a function that given a batch of images and timesteps, produces input-output pairs. Each input will be noised with the corresponding timestep, while the target should be the corresponding noise. Make sure that the returned vector of timesteps is a float32. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 468
        },
        "id": "We8ePeaQgvkv",
        "outputId": "26abb6a2-4a58-4223-a3b7-eaa3bcebecfb"
      },
      "outputs": [],
      "source": [
        "def generate_input_target(batch, schedule):\n",
        "    timesteps = None \n",
        "    noise = None\n",
        "    # TODO\n",
        "    return batch, noise, timesteps.float()\n",
        "\n",
        "# visualise 4 input and target images\n",
        "vis = Visualizer(num_rows=2, num_cols=10, figsize=(20, 5), axis_off=True, title='Input and target images')\n",
        "batch, _ = next(iter(train_loader))\n",
        "input_imgs, targets, timesteps = generate_input_target(batch, schedule)\n",
        "\n",
        "input_imgs = (input_imgs*0.5+0.5).clip(0,1).cpu().numpy().transpose(0, 2, 3, 1)\n",
        "targets = (targets*0.5+0.5).clip(0,1).cpu().numpy().transpose(0, 2, 3, 1)\n",
        "\n",
        "for i in range(10):\n",
        "    vis.add_image_subplot(0, i, input_imgs[i], title_str=f'{timesteps[i,0]}')\n",
        "    vis.add_image_subplot(1, i, targets[i], title_str=f'Target')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We will first write a function to generate samples from the model that we can then use during training.\n",
        "Sampling works like this:\n",
        "\n",
        "\n",
        "* $x_T \\sim \\mathcal{N}(0,1)$\n",
        "* for $t = T$ downto 1:\n",
        "  * $z \\sim \\mathcal{N}(0,1)$ if t > 1 else 0\n",
        "  * $x_{t-1} = \\frac{1}{\\sqrt{\\alpha_t}} \\left( x_t - \\frac{1-\\alpha_t}{\\sqrt{1 - \\bar{\\alpha}_t}}f(x_t, t) \\right) + \\sqrt{\\beta_t}z $\n",
        "\n",
        "Here $f(x_t, t)$ is the model. It takes as input the current sample and the timestep $t$. The loop terminates with $x_0$ which is the final sample.\n",
        "\n",
        "We will \"test\" this function by running it with a randomly initialised model. This will not show us much but checks if the code at least runs :) ...\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 459
        },
        "id": "lHOBjYrogvkw",
        "outputId": "621f5687-5e89-4f87-a4b9-be858f4262bc"
      },
      "outputs": [],
      "source": [
        "def sample(model, schedule):\n",
        "    model.eval()\n",
        "    sample = torch.randn(4, 3, 32, 32, device=schedule.device)\n",
        "\n",
        "    vis = Visualizer(num_rows=sample.shape[0]+3, num_cols=11, figsize=(10, 10), axis_off=True, title='Samples from the model')\n",
        "    with torch.no_grad():\n",
        "        for i in tqdm(range(schedule.num_steps, 0, -1)):\n",
        "            # TODO: compute sample\n",
        "            sample = None\n",
        "            # plotting\n",
        "            if i % (schedule.num_steps // 10) == 0:\n",
        "                img = (sample*0.5 + 0.5).clip(0,1).cpu().numpy().transpose(0, 2, 3, 1)\n",
        "                for j in range(sample.shape[0]):\n",
        "                    vis.add_image_subplot(j, 10 - i//(schedule.num_steps // 10), img[j], title_str=f'{i}')\n",
        "\n",
        "    # plot the final sample\n",
        "    img = (sample*0.5 + 0.5).clip(0,1).cpu().numpy().transpose(0, 2, 3, 1)\n",
        "    for j in range(sample.shape[0]):\n",
        "        vis.add_image_subplot(j, 10, img[j], title_str=f'{0}')\n",
        "\n",
        "    # also show input-target-prediction triplets\n",
        "    batch, _ = next(iter(train_loader))\n",
        "    input_imgs, targets, timesteps = generate_input_target(batch.to(schedule.device), schedule)\n",
        "    with torch.no_grad():\n",
        "        pred = model(input_imgs, timesteps)\n",
        "    input_imgs = (input_imgs*0.5+0.5).clip(0,1).detach().cpu().numpy().transpose(0, 2, 3, 1)\n",
        "    targets = (targets*0.5+0.5).clip(0,1).detach().cpu().numpy().transpose(0, 2, 3, 1)\n",
        "    pred = (pred*0.5+0.5).clip(0,1).cpu().detach().numpy().transpose(0, 2, 3, 1)\n",
        "    for i in range(11):\n",
        "        vis.add_image_subplot(sample.shape[0]+0, i, input_imgs[i], title_str=f'{int(timesteps[i,0])}')\n",
        "        vis.add_image_subplot(sample.shape[0]+1, i, targets[i], title_str=f'Target')\n",
        "        vis.add_image_subplot(sample.shape[0]+2, i, pred[i], title_str=f'pred')\n",
        "    vis.show()\n",
        "\n",
        "sample(Model(), Schedule(1e-4, 0.02, 1000))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Finally, we can write a training loop over multiple epochs.\n",
        "We sample a batch from the dataloader and pass it to `generate_input_target` to get noised images and their corresponding noise targets.\n",
        "We train the model simply with a squared loss on pixel intensities: $\\| \\epsilon_t - f(x_t, t) \\|_2^2$.\n",
        "You will need to train this model for about 500 epochs before the samples start to look like anything. Our network is too small to learn a _great_ diffusion model, but you should see some samples that resemble cars.\n",
        "If possible, connect to a GPU runtime: Edit -> Notebook Settings -> T4 GPU."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 494
        },
        "id": "nmdugfvFgvkw",
        "outputId": "e1ad9e5f-22e9-4f15-ac9c-cae2cf6eaa1c"
      },
      "outputs": [],
      "source": [
        "device=\"cuda:0\"  # use \"cpu\" or \"cuda:0\"\n",
        "model = Model().to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n",
        "lr_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, 1000)  # this will slowly decrease the learning rate for stabilty\n",
        "schedule = Schedule(1e-4, 0.02, 1000, device)\n",
        "\n",
        "epoch_losses = []\n",
        "for epoch in range(1000):\n",
        "    losses = []\n",
        "    model.train()\n",
        "    for batch, _ in tqdm(train_loader):\n",
        "        batch = batch.to(device)\n",
        "        \n",
        "        # TODO: compute targets, and loss\n",
        "        loss = None\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        losses.append(loss.item())\n",
        "    lr_scheduler.step()\n",
        "    model.eval()\n",
        "    print(f'Epoch: {epoch} - Loss: {np.mean(losses):.4f}')\n",
        "    if epoch % 25 == 0:\n",
        "        sample(model, schedule)\n",
        "    \n",
        "\n",
        "print('Training finished')\n",
        "sample(model, schedule)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Optional Task - Classy Diffusion\n",
        "Train a class-conditional model: the model takes the class of a sample as additional input. The easiest way to do that is to change the number of input channels from 3 to 3+10. This means you need to concatenate a \"one-hot\" 10 channel image (where the channels of the corresponding class is all 1s and all other channels are 0). The output of the model remails a 3 channel image. "
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
