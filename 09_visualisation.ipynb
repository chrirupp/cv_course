{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sf_iP5mlKVNh"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import copy\n",
        "import random\n",
        "from tqdm import tqdm\n",
        "import torch\n",
        "import torchvision\n",
        "from torchvision import datasets\n",
        "import torchvision.transforms as transforms\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lFbmkqxWLftk",
        "outputId": "2ec97667-d2a0-4756-c95b-8828cf60cd20"
      },
      "outputs": [],
      "source": [
        "%%sh\n",
        "# Download the data - you need to do this only once\n",
        "wget --no-verbose --output-document=image_dog.jpg https://github.com/chrirupp/cv_course/raw/main/data/image_dog.jpg"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RpVYjfdgKVNh"
      },
      "outputs": [],
      "source": [
        "class Visualizer():\n",
        "    def __init__(self, num_rows=1, num_cols=1, figsize=(5,5), axis_off=True, title='', tight=False, cm=None):\n",
        "        self.fig, self.axs = plt.subplots(num_rows, num_cols, figsize=figsize, squeeze=False)\n",
        "        # remove ticks\n",
        "        if axis_off:\n",
        "          plt.setp(plt.gcf().get_axes(), xticks=[], yticks=[])\n",
        "        # set colormap\n",
        "        if cm is not None:\n",
        "            plt.set_cmap(cm)\n",
        "        # set supertitle\n",
        "        self.fig.suptitle(title)\n",
        "        if tight:\n",
        "            self.fig.subplots_adjust(top=0.88)\n",
        "\n",
        "    def add_image_subplot(self, i, j, image, normalize=False, title_str=''):\n",
        "        if normalize:\n",
        "            image = self.normalize_image(image)\n",
        "        if len(image.shape) == 3:\n",
        "            #BGR -> RGB\n",
        "            image = image[:, :, ::-1]\n",
        "        self.axs[i, j].imshow(image)\n",
        "        self.axs[i, j].set_title(title_str)\n",
        "\n",
        "    def add_stem_subplot(self, i, j, x, y, title_str=''):\n",
        "        self.axs[i, j].stem(x, y)\n",
        "        self.axs[i, j].set_title(title_str)\n",
        "\n",
        "    def add_subplot(self, i, j, data, title_str=''):\n",
        "        self.axs[i, j].plot(data)\n",
        "        self.axs[i, j].set_title(title_str)\n",
        "\n",
        "    def add_bar_subplot(self, i, j, x, y, title_str=''):\n",
        "        self.axs[i, j].bar(x, y)\n",
        "        self.axs[i, j].set_title(title_str)\n",
        "\n",
        "    def add_scatter_subplot_with_labels(self, i, j, data, labels, legend=None, title_str=''):\n",
        "        scatter = self.axs[i, j].scatter(data[:,0], data[:,1], c=labels)\n",
        "        scatter.set_cmap('jet')\n",
        "        if legend is not None:\n",
        "            plt.legend(handles=scatter.legend_elements()[0], labels=legend)\n",
        "        self.axs[i, j].set_title(title_str)\n",
        "\n",
        "    def add_scatter_subplot_with_txt(self, i, j, data, txt, title_str=''):\n",
        "        self.axs[i, j].scatter(data[:,0], data[:,1])\n",
        "        for idx, txt in enumerate(txt):\n",
        "            self.axs[i, j].annotate(txt, (data[idx,0], data[idx,1]))\n",
        "        self.axs[i, j].set_title(title_str)\n",
        "\n",
        "    @staticmethod\n",
        "    def normalize_image(image):\n",
        "        img = np.float64(image) - np.min(image)\n",
        "        img /= np.max(img)\n",
        "        return img"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "go216p59KVNi",
        "outputId": "26fd68fd-90be-425b-e986-4cafd8d71213"
      },
      "outputs": [],
      "source": [
        "# load a resnet18 model\n",
        "model = torch.hub.load('pytorch/vision', 'resnet18', weights=torchvision.models.resnet.ResNet18_Weights.DEFAULT)\n",
        "\n",
        "# visualise first layer filters (this is slow)\n",
        "first_layer_weights = model.conv1.weight.data.cpu()\n",
        "first_layer_weights = first_layer_weights.permute(0, 2, 3, 1).numpy()\n",
        "second_layer_weights = model.layer1[0].conv1.weight.data.cpu().numpy()\n",
        "print(second_layer_weights.shape)\n",
        "\n",
        "size = int(first_layer_weights.shape[0]**0.5)\n",
        "vis = Visualizer(num_rows=size, num_cols=size, figsize=(10,10), axis_off=True, title='First layer weights - ResNet18')\n",
        "for i in range(first_layer_weights.shape[0]):\n",
        "    w = first_layer_weights[i]\n",
        "    w = (w - np.min(w)) / (np.max(w) - np.min(w))\n",
        "    vis.add_image_subplot(i // size, i % size, w, normalize=False)\n",
        "\n",
        "# visualise second layer filters (this is even slower)\n",
        "vis = Visualizer(num_rows=second_layer_weights.shape[1], num_cols=second_layer_weights.shape[0], figsize=(20,10), axis_off=True, title='Second layer weights - ResNet18')\n",
        "for i in range(second_layer_weights.shape[0]):\n",
        "    for j in range(second_layer_weights.shape[1]):\n",
        "        w = second_layer_weights[i, j]\n",
        "        w = (w - np.min(w)) / (np.max(w) - np.min(w))\n",
        "        vis.add_image_subplot(j, i, w, normalize=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "bnsU_EvVKVNi",
        "outputId": "86374d9f-4d45-443e-d75b-b2ac12646a49"
      },
      "outputs": [],
      "source": [
        "# load last layer weights\n",
        "last_layer_weights = model.fc.weight.data.cpu().numpy()\n",
        "# get class names\n",
        "class_names = torchvision.models.resnet.ResNet18_Weights.DEFAULT.meta[\"categories\"]\n",
        "\n",
        "# pca embedding of class vectors\n",
        "from sklearn.decomposition import PCA\n",
        "pca = PCA(n_components=2)\n",
        "pca.fit(last_layer_weights)\n",
        "pca_weights = pca.transform(last_layer_weights)\n",
        "\n",
        "# visualise pca embedding\n",
        "vis = Visualizer(figsize=(20,20), axis_off=True)\n",
        "vis.add_scatter_subplot_with_txt(0, 0, pca_weights, class_names, title_str='PCA embedding of class vectors - ResNet18')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BosuWyH6KVNi",
        "outputId": "ea469742-749f-4b21-cd56-44c969693be6"
      },
      "outputs": [],
      "source": [
        "# load cifar10 dataset\n",
        "cifar10 = datasets.CIFAR10(root='./data', train=False, download=True, transform=transforms.Compose(\n",
        "    [transforms.ToTensor(),\n",
        "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))]))\n",
        "model = torch.hub.load(\"chenyaofo/pytorch-cifar-models\", \"cifar10_resnet20\", pretrained=True)  # change pretrained to False to load random weights and compare the pca/tsne plots\n",
        "\n",
        "# create a dataloader\n",
        "cifar10_loader = torch.utils.data.DataLoader(cifar10, batch_size=100, shuffle=False)\n",
        "\n",
        "# get last layer activations\n",
        "last_layer_activations = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for images, labels in tqdm(cifar10_loader,desc='Computing last layer activations'):\n",
        "\n",
        "        x = model.conv1(images)\n",
        "        x = model.bn1(x)\n",
        "        x = model.relu(x)\n",
        "\n",
        "        x = model.layer1(x)\n",
        "        x = model.layer2(x)\n",
        "        x = model.layer3(x)\n",
        "\n",
        "        x = model.avgpool(x)\n",
        "        activations = x.view(x.size(0), -1).cpu().detach().numpy()\n",
        "        last_layer_activations.append(activations)\n",
        "last_layer_activations = np.concatenate(last_layer_activations, axis=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Wogx-YPxKVNj",
        "outputId": "cb670448-093e-4a2b-fee0-ada298350437"
      },
      "outputs": [],
      "source": [
        "pca = PCA(n_components=2)\n",
        "pca.fit(last_layer_activations)\n",
        "pca_activations = pca.transform(last_layer_activations)\n",
        "\n",
        "# visualise pca embedding\n",
        "vis = Visualizer(figsize=(10,10), axis_off=True)\n",
        "vis.add_scatter_subplot_with_labels(0, 0, pca_activations, cifar10.targets, legend=cifar10.classes, title_str='PCA embedding of last layer activations - random init ResNet20')\n",
        "\n",
        "# visualise tsne embedding\n",
        "from sklearn.manifold import TSNE\n",
        "tsne = TSNE(n_components=2, perplexity=30, n_iter=300)\n",
        "tsne_activations = tsne.fit_transform(last_layer_activations)\n",
        "vis = Visualizer(figsize=(10,10), axis_off=True)\n",
        "vis.add_scatter_subplot_with_labels(0, 0, tsne_activations, cifar10.targets, legend=cifar10.classes, title_str='t-SNE embedding of last layer activations - random init ResNet20')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 601
        },
        "id": "wUlZwDKBKVNj",
        "outputId": "a596f200-d277-4178-b4dd-8f7ee307d196"
      },
      "outputs": [],
      "source": [
        "# load different models\n",
        "\n",
        "model = torch.hub.load('pytorch/vision', 'resnet18', weights=None)\n",
        "model = torch.hub.load('pytorch/vision', 'resnet18', weights=torchvision.models.resnet.ResNet18_Weights.DEFAULT)\n",
        "#class_names = torchvision.models.resnet.ResNet18_Weights.DEFAULT.meta[\"categories\"]\n",
        "\n",
        "# model = torch.hub.load('pytorch/vision', 'resnet50', weights=torchvision.models.resnet.ResNet50_Weights.DEFAULT)\n",
        "# class_names = torchvision.models.resnet.ResNet18_Weights.DEFAULT.meta[\"categories\"]\n",
        "\n",
        "# model = torch.hub.load('pytorch/vision', 'vgg16', weights=torchvision.models.vgg.VGG16_Weights.DEFAULT)\n",
        "# class_names = torchvision.models.vgg.VGG16_Weights.DEFAULT.meta[\"categories\"]\n",
        "\n",
        "# model = torch.hub.load('pytorch/vision', 'alexnet', weights=None)\n",
        "# model = torch.hub.load('pytorch/vision', 'alexnet', weights=torchvision.models.AlexNet_Weights.DEFAULT)\n",
        "#class_names = torchvision.models.AlexNet_Weights.DEFAULT.meta[\"categories\"]\n",
        "\n",
        "#model = torch.hub.load('pytorch/vision', 'vit_l_16', weights=torchvision.models.ViT_L_16_Weights.IMAGENET1K_V1)\n",
        "#class_names = torchvision.models.ViT_L_16_Weights.IMAGENET1K_V1.meta[\"categories\"]\n",
        "\n",
        "# load image\n",
        "image = cv2.imread('image_dog.jpg')\n",
        "\n",
        "# preprocess image\n",
        "#crop to square on the left\n",
        "image = image[:, :image.shape[0]]\n",
        "image = cv2.resize(image, (224, 224))\n",
        "vis = Visualizer(num_rows=1, num_cols=1, figsize=(5,5), axis_off=True, title='Input image')\n",
        "vis.add_image_subplot(0, 0, image)\n",
        "image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "image = image / 255.0\n",
        "image = image - np.array([0.485, 0.456, 0.406])\n",
        "image = image / np.array([0.229, 0.224, 0.225])\n",
        "image = image.transpose(2, 0, 1)\n",
        "image = np.expand_dims(image, axis=0)\n",
        "image = torch.from_numpy(image).float()\n",
        "\n",
        "# get prediction\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    prediction = model(image).detach().cpu().numpy()\n",
        "\n",
        "# show top 5 predictions\n",
        "probabilities = np.exp(prediction) / np.sum(np.exp(prediction))\n",
        "top5 = np.argsort(-prediction, axis=1)[:, :5]\n",
        "for i in range(5):\n",
        "    print(f'{i+1}. class: {class_names[top5[0, i]]} probability: {probabilities[0, top5[0, i]]:.2f}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 944
        },
        "id": "TFRga1kTKVNj",
        "outputId": "2db73298-29f9-4aad-9db1-93df7d5e08f4"
      },
      "outputs": [],
      "source": [
        "# occlusion method\n",
        "\n",
        "def get_response(image, model, x, y, size=50):\n",
        "    occ = np.copy(image)\n",
        "    occ[0, :, x:x+size, y:y+size] = 0\n",
        "    with torch.no_grad():\n",
        "        response = model(torch.from_numpy(occ)).detach().cpu().numpy()\n",
        "    return response\n",
        "\n",
        "# get heatmap\n",
        "heatmap = np.zeros((224, 224))\n",
        "# get prediction\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    original_prediction = model(image).detach().cpu().numpy()\n",
        "\n",
        "size=15\n",
        "top_class = top5[0, 0]\n",
        "print(f'Class: {class_names[top_class]}')\n",
        "for x in tqdm(range(0, 224, size)):\n",
        "    for y in range(0, 224, size):\n",
        "        response = get_response(image, model, x, y, size)\n",
        "        heatmap[x:x+size, y:y+size] = (original_prediction[0, top_class] - response[0, top_class])**2\n",
        "\n",
        "# normalise heatmap\n",
        "heatmap = (heatmap - np.min(heatmap)) / (np.max(heatmap) - np.min(heatmap))\n",
        "vis = Visualizer(num_rows=1, num_cols=1, figsize=(10,10), axis_off=True, title='Occlusion heatmap')\n",
        "vis.add_image_subplot(0, 0, heatmap, )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 907
        },
        "id": "jYdYfGhaKVNj",
        "outputId": "f54d26ed-81b7-4be7-e93d-bbd3e3ade6a5"
      },
      "outputs": [],
      "source": [
        "# gradient method\n",
        "\n",
        "def get_gradient(image, model, top_class, num_samples=1):\n",
        "    gradients = []\n",
        "    for i in range(num_samples):\n",
        "        input = torch.from_numpy(np.copy(image)).float()\n",
        "        if num_samples > 1:\n",
        "            input += torch.normal(0, 0.1, image.shape)\n",
        "        input.requires_grad = True\n",
        "        prediction = model(input)\n",
        "        prediction[0, top_class].backward()\n",
        "        gradients.append(input.grad.detach().cpu().numpy().max(axis=1))\n",
        "    gradient = np.mean(np.concatenate(gradients, axis=0), axis=0)\n",
        "    # gradient = np.var(np.concatenate(gradients, axis=0), axis=0)\n",
        "    return gradient\n",
        "\n",
        "# get gradient\n",
        "top_class = top5[0, 0]\n",
        "gradient = get_gradient(image, model, top_class)\n",
        "gradient = (gradient - np.min(gradient)) / (np.max(gradient) - np.min(gradient))\n",
        "vis = Visualizer(num_rows=1, num_cols=1, figsize=(10,10), axis_off=True, title='Gradient heatmap')\n",
        "vis.add_image_subplot(0, 0, gradient)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 490
        },
        "id": "pwuZecTrKVNj",
        "outputId": "956dce2d-fb25-4ec6-d5ea-7cb3a1f526d2"
      },
      "outputs": [],
      "source": [
        "# input maximisation\n",
        "\n",
        "def normalise_image(x):\n",
        "    x = x.detach().cpu().numpy()\n",
        "    x = x[0].transpose(1, 2, 0)\n",
        "    x = (x - np.min(x)) / (np.max(x) - np.min(x))\n",
        "    return x\n",
        "\n",
        "image = np.random.rand(1, 3, 224, 224)*0.1+0.45\n",
        "image = torch.from_numpy(image).float()\n",
        "image = torch.nn.Parameter(image, requires_grad=True)\n",
        "optimizer = torch.optim.Adam([image], lr=0.1)\n",
        "\n",
        "top_class = 207\n",
        "model.eval()\n",
        "\n",
        "num_iterations = 300\n",
        "imgnet_mean = torch.from_numpy(np.array([0.485, 0.456, 0.406])).float()\n",
        "imgnet_std = torch.from_numpy(np.array([0.229, 0.224, 0.225])).float()\n",
        "tv_losses = []\n",
        "class_scores = []\n",
        "for i in tqdm(range(num_iterations)):\n",
        "    inputs = []\n",
        "    for i in range(3):\n",
        "        roll = (random.randint(-8, 8), random.randint(-8, 8))\n",
        "        input = torch.roll(image, shifts=roll, dims=(2,3))\n",
        "        inputs.append(input)\n",
        "    input = torch.concatenate(inputs, axis=0)\n",
        "    input = (input - imgnet_mean[None, :, None, None]) / imgnet_std[None, :, None, None]\n",
        "    prediction = model(input)\n",
        "    tv_loss = torch.sum(torch.abs(input[:, :, :, :-1] - input[:, :, :, 1:])) + torch.sum(torch.abs(input[:, :, :-1, :] - input[:, :, 1:, :]))\n",
        "    tv_losses.append(tv_loss.detach().cpu().numpy())\n",
        "    class_scores.append(prediction[0, top_class].detach().cpu().numpy())\n",
        "    loss = -prediction[0, top_class] + 0.06 * tv_loss\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    # gradient clipping\n",
        "    torch.nn.utils.clip_grad_norm_(image, 0.1)\n",
        "    optimizer.step()\n",
        "    image.data = torch.clamp(image, 0, 1)\n",
        "\n",
        "vis = Visualizer(num_rows=1, num_cols=3, figsize=(15,5), axis_off=False, title=f'Input maximisation class {class_names[top_class]}')\n",
        "vis.add_image_subplot(0, 0, normalise_image(image))\n",
        "vis.add_subplot(0, 1, class_scores, title_str='Class score')\n",
        "vis.add_subplot(0, 2, tv_losses, title_str='TV loss')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "docu7TRIKVNj",
        "outputId": "5e998de4-e043-4390-b575-b940a690ec72"
      },
      "outputs": [],
      "source": [
        "# attention visualisation\n",
        "\n",
        "# load a vit model\n",
        "#model = torch.hub.load('pytorch/vision', 'vit_base_patch16_224', pretrained=True)\n",
        "#class_names = torchvision.models.vit_base_patch16_224(pretrained=True).meta[\"categories\"]\n",
        "model = torch.hub.load('pytorch/vision', 'vit_b_16', weights=torchvision.models.ViT_B_16_Weights.IMAGENET1K_V1)\n",
        "class_names = torchvision.models.ViT_B_16_Weights.IMAGENET1K_V1.meta[\"categories\"]\n",
        "\n",
        "# load image\n",
        "image = cv2.imread('image_dog.jpg')\n",
        "\n",
        "# preprocess image\n",
        "image = image[:, :image.shape[0]]\n",
        "image = cv2.resize(image, (224, 224))\n",
        "vis = Visualizer(num_rows=1, num_cols=1, figsize=(5,5), axis_off=True, title='Input image')\n",
        "vis.add_image_subplot(0, 0, image)\n",
        "image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "image = image / 255.0\n",
        "image = image - np.array([0.485, 0.456, 0.406])\n",
        "image = image / np.array([0.229, 0.224, 0.225])\n",
        "image = image.transpose(2, 0, 1)\n",
        "image = np.expand_dims(image, axis=0)\n",
        "image = torch.from_numpy(image).float()\n",
        "\n",
        "\n",
        "# get prediction\n",
        "with torch.no_grad():\n",
        "    x = model._process_input(image)\n",
        "    n = x.shape[0]\n",
        "    batch_class_token = model.class_token.expand(n, -1, -1)\n",
        "    x = torch.cat([batch_class_token, x], dim=1)\n",
        "\n",
        "    input = x + model.encoder.pos_embedding\n",
        "    input = model.encoder.dropout(input)\n",
        "\n",
        "    attention = []\n",
        "    for i, layer in enumerate(model.encoder.layers):\n",
        "        x = layer.ln_1(input)\n",
        "        x, atn_weights = layer.self_attention(x, x, x, need_weights=True, average_attn_weights=True)\n",
        "        n = int(np.sqrt(atn_weights.shape[2]-1))\n",
        "        atn_weights = atn_weights[:, 1:, 1:].view(n, n, n, n)\n",
        "        attention.append(atn_weights.cpu().detach().numpy())\n",
        "        x = layer.dropout(x)\n",
        "        x = x + input\n",
        "\n",
        "        y = layer.ln_2(x)\n",
        "        y = layer.mlp(y)\n",
        "        input = x + y\n",
        "    x = model.encoder.ln(input)\n",
        "    x = x[:, 0]\n",
        "    prediction = model.heads(x).detach().cpu().numpy()\n",
        "\n",
        "# show top 5 predictions\n",
        "probabilities = np.exp(prediction) / np.sum(np.exp(prediction))\n",
        "top5 = np.argsort(-prediction, axis=1)[:, :5]\n",
        "for i in range(5):\n",
        "    print(f'{i+1}. class: {class_names[top5[0, i]]} probability: {probabilities[0, top5[0, i]]:.2f}')\n",
        "\n",
        "# visualise attention maps\n",
        "vis = Visualizer(num_rows=1, num_cols=len(model.encoder.layers), figsize=(120,10), axis_off=True, title='Attention maps')\n",
        "for i in range(len(attention)):\n",
        "    vis.add_image_subplot(0, i, attention[i][:, :, 7, 7], normalize=True, title_str=f'Layer {i+1}')\n",
        "\n",
        "vis = Visualizer(num_rows=1, num_cols=len(model.encoder.layers), figsize=(120,10), axis_off=True, title='Attention maps')\n",
        "for i in range(len(attention)):\n",
        "    vis.add_image_subplot(0, i, attention[i][:, :, 1, 1], normalize=True, title_str=f'Layer {i+1}')\n",
        "\n",
        "vis = Visualizer(num_rows=1, num_cols=len(model.encoder.layers), figsize=(120,10), axis_off=True, title='Attention maps')\n",
        "for i in range(len(attention)):\n",
        "    vis.add_image_subplot(0, i, attention[i][:, :, 6, 12], normalize=True, title_str=f'Layer {i+1}')\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "cvlecture",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
