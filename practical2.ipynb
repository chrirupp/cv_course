{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_3c6XOXzC0r_"
      },
      "source": [
        "# Computer Vision HT 2025 - Practical 2 (v1.0)\n",
        "\n",
        "## Instructions\n",
        "\n",
        "1. Start Google Colab: https://colab.research.google.com. A modal dialog should have appeared to open a new notebook. If not, go to \"File>Open notebook\".\n",
        "2. From the open notebook dialog, select the GitHub \"tab\" and enter this URL: https://github.com/chrirupp/cv_course\n",
        "3. The notebook(s) should appear (*.ipynb). Select the one for the current practical.\n",
        "4. To run a notebook on Colab you will typically need some data files (e.g., images). As Colab only loads the notebook itself, these other files need to be downloaded separately. The following cell is a `%%sh` block that downloads the required files. You can inspect the downloaded files by clicking on the \"Files\" tab on the left.\n",
        "\n",
        "## Practicalities\n",
        "\n",
        "The signing-off happens in the last half hour of each session or at the beginning of the following one.\n",
        "As usual, when checking your work the demonstrator will want to see a working version of the program in action, as well as appropriate comments of your code. Try to make your report as concise as possible, perhaps in the form of appropriate comments to your code.\n",
        "\n",
        "Since this is a new practical task, any errors, ambiguities or suggestions for improvement should be flagged as soon as possible.\n",
        "\n",
        "If you are not familiar with the way practicals run, there are department-wide [rules](https://www.cs.ox.ac.uk/teaching/courses/2023-2024/practicals/). There you will find how the compulsory part, the optional tasks, and your report will factor into your mark.\n",
        "\n",
        "## Advice\n",
        "\n",
        "* You will need to look at the code for the lectures. There you will find many related computations that you can reuse and adapt to solve the practicals.\n",
        "* The compulsory part of this practical is designed to give you additional understanding of the concepts taught in the lectures. It should be achievable in one session."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ryes_wMLC0sA"
      },
      "source": [
        "Here we import some libraries that we will need to process images, do maths, and to visualise results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "RXPyM6HfC0sB"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import torchvision\n",
        "import torch\n",
        "from tqdm import tqdm\n",
        "from torchvision import datasets, transforms\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VWYKALggC0sB"
      },
      "source": [
        "The usual set of helper functions:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NwoV1uadC0sB",
        "outputId": "042a7812-03be-4561-9a09-03b98e56f5e7"
      },
      "outputs": [],
      "source": [
        "%%sh\n",
        "# Download the data - you need to do this only once\n",
        "wget --no-verbose --output-document=image_dog.jpg https://github.com/chrirupp/cv_course/raw/main/data/image_dog.jpg"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eoF6NEBtC0sB"
      },
      "outputs": [],
      "source": [
        "class Visualizer():\n",
        "    def __init__(self, num_rows=1, num_cols=1, figsize=(5,5), axis_off=True, title='', tight=False, cm=None):\n",
        "        fig, self.axs = plt.subplots(num_rows, num_cols, figsize=figsize, squeeze=False)\n",
        "        # remove ticks\n",
        "        if axis_off:\n",
        "          plt.setp(plt.gcf().get_axes(), xticks=[], yticks=[])\n",
        "        # set colormap\n",
        "        if cm is not None:\n",
        "            plt.set_cmap(cm)\n",
        "        # set supertitle\n",
        "        fig.suptitle(title)\n",
        "        if tight:\n",
        "            fig.subplots_adjust(top=0.88)\n",
        "\n",
        "    def add_image_subplot(self, i, j, image, normalize=False, title_str=''):\n",
        "        if normalize:\n",
        "            image = self.normalize_image(image)\n",
        "        if len(image.shape) == 3:\n",
        "            #BGR -> RGB\n",
        "            image = image[:, :, ::-1]\n",
        "        self.axs[i, j].imshow(image)\n",
        "        self.axs[i, j].set_title(title_str)\n",
        "\n",
        "    def add_stem_subplot(self, i, j, x, y, title_str=''):\n",
        "        self.axs[i, j].stem(x, y)\n",
        "        self.axs[i, j].set_title(title_str)\n",
        "\n",
        "    def add_subplot(self, i, j, data, title_str=''):\n",
        "        self.axs[i, j].plot(data)\n",
        "        self.axs[i, j].set_title(title_str)\n",
        "\n",
        "    def add_bar_subplot(self, i, j, x, y, title_str=''):\n",
        "        self.axs[i, j].bar(x, y)\n",
        "        self.axs[i, j].set_title(title_str)\n",
        "\n",
        "    @staticmethod\n",
        "    def normalize_image(image):\n",
        "        img = np.float64(image) - np.min(image)\n",
        "        img /= np.max(img)\n",
        "        return img"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dHf5boyYC0sB"
      },
      "source": [
        "## Problem 2.1 - Image Classification\n",
        "\n",
        "Let us train a linear classifier on the CIFAR10 dataset.\n",
        "First, we will load the dataset and show some sample images. The convention in pytorch is to store images in a tensor of shape $C\\times H \\times W$, where $C=3$ is the number of channels and $H$ and $W$ are the height and width of the image. For training, we typically load mini-batches of size $N$, which adds another dimension $N \\times C \\times H \\times W$ to our data. The image visualiser expects data in the format $H \\times W \\times C$ so you will find some shuffling of dimension in the code below. Additionally, OpenCV prefers processing images in BGR format, while pytorch uses RGB so we also need to reverse the order of the $C$ dimension."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yqwHS8v6C0sB"
      },
      "outputs": [],
      "source": [
        "# get cifar10 dataset\n",
        "transform = transforms.Compose([transforms.ToTensor()])\n",
        "train_dataset = datasets.CIFAR10('../data', train=True, download=True, transform=transform)\n",
        "test_dataset = datasets.CIFAR10('../data', train=False, download=True, transform=transform)\n",
        "\n",
        "\n",
        "# samples from cifar10 dataset\n",
        "vis = Visualizer(3, 10, figsize=(25,5), title='CIFAR10 samples')\n",
        "for i in range(30):\n",
        "    image = train_dataset[i][0].permute(1, 2, 0).numpy()[:,:,::-1]\n",
        "    vis.add_image_subplot(i // 10, i % 10, image, title_str=train_dataset.classes[train_dataset[i][1]])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H4RxfPoqC0sB"
      },
      "source": [
        "While we can \"train\" a linear classifier by solving a linear system analytically, we will now do it with gradient descent to familiarise ourselves with the concepts of training. Training typically happens in epochs, where one epoch is a full loop over the training set. Familiarise yourself with the training code below. Answer the following questions:\n",
        "\n",
        "* What is the purpose of `optimizer.zero_grad()`?\n",
        "* What is `citerion`?\n",
        "* How do we determine which class the model predicts?\n",
        "* How many parameters does our model have?\n",
        "* What accuracy would we expect for a model that is randomly initialised?\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SAWTkxokC0sC"
      },
      "outputs": [],
      "source": [
        "# create dataloaders\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
        "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=128, shuffle=False)\n",
        "\n",
        "def evaluate(model, test_loader):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    with torch.no_grad():\n",
        "        for data, target in test_loader:\n",
        "            output = model(data)\n",
        "            pred = output.argmax(dim=1, keepdim=True)\n",
        "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "\n",
        "    accuracy = 100. * correct / len(test_loader.dataset)\n",
        "    return accuracy\n",
        "\n",
        "def train(model, train_loader, test_loader, criterion, optimizer, num_epochs=10):\n",
        "    train_losses = []\n",
        "    test_accuracies = []\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        for data, target in tqdm(train_loader):\n",
        "            optimizer.zero_grad()\n",
        "            output = model(data)\n",
        "            loss = criterion(output, target)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            train_losses.append(loss.item())\n",
        "\n",
        "        test_acc = evaluate(model, test_loader)\n",
        "        test_accuracies.append(test_acc)\n",
        "        print(f'Epoch {epoch+1}/{num_epochs}, Test accuracy: {test_acc:.2f}%')\n",
        "\n",
        "    return train_losses, test_accuracies\n",
        "\n",
        "class LinearModel(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.fc = torch.nn.Linear(3*32*32, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.view(-1, 3*32*32)\n",
        "        x = self.fc(x)\n",
        "        return x\n",
        "\n",
        "model = LinearModel()\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "train_losses, test_accuracies = train(model, train_loader, test_loader, criterion, optimizer, num_epochs=5)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W78-ICISC0sC"
      },
      "source": [
        "Now that the model is trained, visualise the weights of the linear layer. There are $3\\cdot 32 \\cdot 32$ weights for each class which you can visualise simply as an image. What do you see? Why does it look like that?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z1EjTugFC0sC"
      },
      "outputs": [],
      "source": [
        "vis = Visualizer(1, 10, figsize=(10,2), title='Weights')\n",
        "for i in range(10):\n",
        "    weights = 0\n",
        "    # TODO: visualise weights\n",
        "    vis.add_image_subplot(0, i, weights, title_str=train_dataset.classes[i], normalize=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OsY5gMZpC0sC"
      },
      "source": [
        "We will now train a small CNN. It consists of\n",
        "* Convolution: size 3x3, number of channels 3 (RGB), number of filters: 16, padding: 1 on all sides\n",
        "* ReLU\n",
        "* MaxPooling: size 2x2, stride: 2\n",
        "* Fully Connected layer: 4096 x 10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "erVV9POdC0sC"
      },
      "outputs": [],
      "source": [
        "class MiniCNN(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.conv1 = torch.nn.Conv2d(3, 16, kernel_size=3, stride=1, padding=1)\n",
        "        self.fc = torch.nn.Linear(4096, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.nn.functional.relu(self.conv1(x))\n",
        "        x = torch.nn.functional.max_pool2d(x, kernel_size=2, stride=2)\n",
        "        x = x.view(-1, 4096)\n",
        "        x = self.fc(x)\n",
        "        return x\n",
        "\n",
        "model = MiniCNN()\n",
        "# TODO: train the model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tC_RKB2_C0sC"
      },
      "source": [
        "This model should achieve around 57% accuracy on the test set (with the default settings for batch size, learning rate, and number of epochs).\n",
        "\n",
        "Answer the following questions:\n",
        "* Why do we need to define the convolution and FC layer in the constructor (`__init__()`) of the model, but ReLU and max_pool are simply used in the forward pass?\n",
        "* How do we know that the fully connected layer needs to be of size 4096 x 10?\n",
        "\n",
        "Then, expand the model with a second set of convolution (3x3, 32 filters, padding 1), ReLU, max_pool (2x2, stride 2). You will need to also adjust the size of the FC layer accordingly. Confirm that this improves the accuracy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bo4SWrvFC0sC"
      },
      "outputs": [],
      "source": [
        "class MiniCNN2(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        # TODO: define the model\n",
        "\n",
        "    def forward(self, x):\n",
        "        # TODO: define the forward pass\n",
        "        return x\n",
        "\n",
        "model = MiniCNN2()\n",
        "# TODO: train the model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AWff9E48C0sC"
      },
      "source": [
        "### Optional Task - Be a hero\n",
        "You have now several options that might improve the performance:\n",
        "* Change the architecture further.\n",
        "* Train longer.\n",
        "* Change other hyper parameters: batch size, learning rate, kernel sizes, number of filters, activation functions, etc.\n",
        "* Data augmentations such as left-right flipping (see `transform` in the beginning).\n",
        "\n",
        "Try to find a combination that gets better accuracy than `MiniCNN2` (at least 63%). Do not change the test set! (State-of-the-art models get around 95% ;) )\n",
        "Hint: if you are considering to tinker with this for a while: switch to hardware acceleration (GPU) - Edit->Notebook settings. Then change the code to move the model and tensors to the GPU `.to(\"cuda\")` before starting to train. You will need to move each batch to the device too. Move predictions back to the CPU `.cpu()` so that they do not accumulate memory."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qbNyRhO0C0sC"
      },
      "source": [
        "## Problem 2.2 - Occlusion Method\n",
        "\n",
        "The code for the lecture implements the occlusion method for CNNs, by blocking out (i.e. setting to 0) parts of the image and measuring the change in response for the target class for every location, and converting it into a heatmap.\n",
        "We will now do the same for a transformer architecture. However, since the transformer already divides the image into patches, we can directly block out tokens instead of modifying the image."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gTq8uNc1C0sC"
      },
      "outputs": [],
      "source": [
        "model = torch.hub.load('pytorch/vision', 'vit_b_16', weights=torchvision.models.ViT_B_16_Weights.IMAGENET1K_V1)\n",
        "class_names = torchvision.models.ViT_B_16_Weights.IMAGENET1K_V1.meta[\"categories\"]\n",
        "\n",
        "# load image\n",
        "image = cv2.imread('image_dog.jpg')\n",
        "\n",
        "# preprocess image\n",
        "image = image[:, :image.shape[0]]\n",
        "image = cv2.resize(image, (224, 224))\n",
        "vis = Visualizer(num_rows=1, num_cols=1, figsize=(5,5), axis_off=True, title='Input image')\n",
        "vis.add_image_subplot(0, 0, image)\n",
        "image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "image = image / 255.0\n",
        "image = image - np.array([0.485, 0.456, 0.406])\n",
        "image = image / np.array([0.229, 0.224, 0.225])\n",
        "image = image.transpose(2, 0, 1)\n",
        "image = np.expand_dims(image, axis=0)\n",
        "image = torch.from_numpy(image).float()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EFxDx7I0C0sC"
      },
      "source": [
        "Models trained on the ImageNet dataset are trained using normalised images (subtract mean, divide by variance) this is reflected in the image pre-processing code above.\n",
        "\n",
        "Below you are given a function `predict`.\n",
        "\n",
        "Explain:\n",
        "* the role of `batch_class_token`\n",
        "* `encoder.pos_embedding`\n",
        "* the reason for `x[:, 0]`\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8irqO-K_C0sC"
      },
      "outputs": [],
      "source": [
        "def predict(model, image):\n",
        "    x = model._process_input(image)\n",
        "    n = x.shape[0]\n",
        "    batch_class_token = model.class_token.expand(n, -1, -1)\n",
        "    x = torch.cat([batch_class_token, x], dim=1)\n",
        "\n",
        "    input = x + model.encoder.pos_embedding\n",
        "    input = model.encoder.layers(input)\n",
        "    x = model.encoder.ln(input)\n",
        "    x = x[:, 0]\n",
        "    return model.heads(x).detach().cpu().numpy()\n",
        "\n",
        "# predict\n",
        "output = predict(model, image)\n",
        "pred = class_names[output.argmax()]\n",
        "print(f'Predicted class: {pred}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UE3e2SFVC0sC"
      },
      "source": [
        "Implement the occlusion method by individually \"occluding\" image token from the input to the encoder and measuring the change in the target class. Be careful: the class-token is the first token in the input sequence. Visualise the heatmap."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VI8UBVxHC0sD"
      },
      "outputs": [],
      "source": [
        "def predict_drop_token(model, image, token):\n",
        "    # TODO: implement the function\n",
        "    return None\n",
        "\n",
        "# get prediction\n",
        "with torch.no_grad():\n",
        "    original_prediction = 0 # TODO: get the original prediction\n",
        "\n",
        "top_class = original_prediction.argmax()\n",
        "print(f'Class: {class_names[top_class]}')\n",
        "\n",
        "heatmap = np.zeros((?, ?))\n",
        "for i in tqdm(range(heatmap.size)):\n",
        "    response = predict_drop_token(model, image, i)\n",
        "    heatmap[?, ?] = 0  # TODO: calculate the heatmap\n",
        "\n",
        "# plot heatmap\n",
        "vis = Visualizer(num_rows=1, num_cols=1, figsize=(5,5), axis_off=True, title='Heatmap')\n",
        "vis.add_image_subplot(0, 0, heatmap)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cIdBfPjGC0sD"
      },
      "source": [
        "## Optional Task - Visualise Everything\n",
        "Upload some images and show predictions and heatmaps. Are their clear failure modes? What happens if you visualise a class that is not in the image?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ld2felXbC0sD"
      },
      "source": [
        "## Problem 2.3 - Adversarial Examples\n",
        "\n",
        "Adversarial examples are inputs to the network that look like normal images to humans, but cause the network to predict completely wrong classes with very high confidence. The are called _adversarial_ since they are specifically constructed to fool the model while being imperceptible to humans.\n",
        "\n",
        "A straightforward way to construct adversarial examples is a process very similar to the Input Reconstruction visualisation technique. However, instead of starting the optimisation from random noise, we start from the image that we want to modify. We then make small gradient steps in the image that maximise our target adversarial class. Here, we do not need any regularisers since we are looking for a solution very close to the original image.\n",
        "\n",
        "Take the input reconstruction code from the lecture notebook on visualisation as an example on how to setup gradient descent on pixels, given a network and target class.\n",
        "* You will not need any regularisation techniques.\n",
        "* you also do not need to normalise the gradient.\n",
        "* start from image_dog instead of random noise.\n",
        "* choose a target class (e.g. 637 \"mailbox\")\n",
        "* optimise and show your adversarial example.\n",
        "* pass your adversarial example through the model and show that it predicts the target class.\n",
        "\n",
        "Hint: you will need to normalise the image (like in the code above). When visualising the result, you need to invert this process."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MjaqowCeC0sD"
      },
      "outputs": [],
      "source": [
        "# load image\n",
        "image = cv2.imread('image_dog.jpg')\n",
        "\n",
        "# preprocess image\n",
        "image = image[:, :image.shape[0]]\n",
        "image = cv2.resize(image, (224, 224))\n",
        "vis = Visualizer(num_rows=1, num_cols=1, figsize=(5,5), axis_off=True, title='Input image')\n",
        "vis.add_image_subplot(0, 0, image)\n",
        "image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "image = image / 255.0\n",
        "image = image - np.array([0.485, 0.456, 0.406])\n",
        "image = image / np.array([0.229, 0.224, 0.225])\n",
        "image = image.transpose(2, 0, 1)\n",
        "image = np.expand_dims(image, axis=0)\n",
        "image = torch.from_numpy(image).float()\n",
        "\n",
        "def de_normalise_image(x):\n",
        "    # TODO: denormalise image (test it by using it on the normalised image and plot it)\n",
        "    return x\n",
        "\n",
        "model = torch.hub.load('pytorch/vision', 'resnet50', weights=torchvision.models.resnet.ResNet50_Weights.DEFAULT)\n",
        "class_names = torchvision.models.resnet.ResNet18_Weights.DEFAULT.meta[\"categories\"]\n",
        "\n",
        "# pass image through model\n",
        "output = model(image)\n",
        "pred = output.argmax()\n",
        "print(f'Predicted class: {class_names[pred]}')\n",
        "\n",
        "top_class = 637 # This is the class for 'mailbox'\n",
        "num_iterations = 50\n",
        "input = 0  # TODO: initialise with image\n",
        "input = torch.nn.Parameter(input, requires_grad=True)\n",
        "optimizer = torch.optim.Adam([input], lr=0.1)\n",
        "\n",
        "for i in tqdm(range(num_iterations)):\n",
        "    prediction = model(input)\n",
        "    loss = 0  # TODO: find a loss function that maximises the class score\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "vis = Visualizer(num_rows=1, num_cols=1, figsize=(5,5), axis_off=True, title=f'Input maximisation class {class_names[top_class]}')\n",
        "vis.add_image_subplot(0, 0, de_normalise_image(image))\n",
        "\n",
        "# pass image through model\n",
        "output = model(input)\n",
        "# get class index\n",
        "pred = output.argmax()\n",
        "print(f'Predicted class of adversarial example: {class_names[pred]}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5CUZ5awTC0sD"
      },
      "source": [
        "Visualise the difference image between the original and the adversarial example.\n",
        "* Do the same for different classes.\n",
        "* Modify the code so that the loop ends as soon as the target class is higher than all others. Are some classes \"easier\" than others?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tz4jCzN2C0sD"
      },
      "source": [
        "## Optional Tasks\n",
        "* Save and load an adversarial example to and from a jpg file. Does it still work? Why? How could you make it more robust?\n",
        "* Universal perturbations: another curious finding is that there are universal perturbations. A UP is a noise pattern $\\epsilon$ that, if added to _any_ image, causes a misclassification as the target class. Try to find a universal perturbation for one of your CIFAR10 CNNs from the first problem of this sheet. You can do this by running the learning of the perturbation over batches of the training set. For each image in the batch, you will need to add your universal perturbation. The only thing that is learned in this training loop is the pattern (the model stays fixed). Learn your universal perturbation on the training set and show some examples on the test set. How do they compare to adversarial examples?\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "cvlecture",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
